Luigi-nervous-controls: W-Where am I?

Luigi-scared-hutaospook: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA!!!!!!!!!!!!!!!!

HuTao-ghost-hutaospook: Hello Luigi, I am HuTao, the 77th Director of the Wangsheng Funeral Parlor. I have been sent by the Adventurers' Guild to teach you about principal component analysis. 

Luigi-scared: PLEASE SPARE ME. 

HuTao-unamused: ...

HuTao-unamused: I am not here to harm you. I am here to teach you. 

Luigi-scared: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA!!!!!!!!!!!!!!!!

HuTao-unamused: Anyhow... let's get started...

HuTao-ghost: Lets say that we have some images of faces which we want to analyze. 
HuTao-ghost: We could start off by going through each and every image to figure out similarities, differences, or patterns. 
HuTao-ghost: If there are only a few images, it should be pretty straightforward.
HuTao-ghost: Hover over the images to get a closer look. From the few images you see here, there should be no problem in quickly seeing that there are clear artistic styles. 
HuTao-ghost: But...
HuTao-ghost: But... what if we have hundreds...
HuTao-ghost: But... what if we have hundreds... thousands...
HuTao-spook: But... what if we have hundreds... thousands... or even TENS OF THOUSANDS of images?

Luigi-scared: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA!!!!!!!!!!!!!!!!

HuTao-ghost: Yes, yes, truly a nightmare. I'll give you a moment to immerse yourself in such horror.
HuTao-ghost:

HuTao-ghost: As you can probably tell, this task would be extremely time-consuming and tedious.
HuTao-ghost: However, what if I told you that there is a way to quickly gain insight into the dataset without having to go through each and every image?
HuTao-ghost: A way to go about this is through reducing the dimensionality of the images to 2 or 3 dimensions.
HuTao-ghost: This makes it so that you can can create a plot of all of the image representations, and quickly glean insights. 

HuTao-ghost: However, as you might imagine, there are trade-offs. 
HuTao-ghost: Our images are 64x64 grayscale, which means that there are 4096 values are associated with each image.
HuTao-ghost: To put it more formally, our images are 4096 dimensional vectors. 
HuTao-ghost: If we want a 2 dimensional representation, that means we'll be losing 4094 dimensions of image information. 
HuTao-ghost: Because of this, it's imperative that we find a representation that *minimizes the information lost*.

HuTao-ghost: To build some intuition for this idea, let's consider a simple example of finding good 1-dimensional representations of a dataset of 2-dimensional vectors.
HuTao-ghost: All of the blue datapoints are projected onto the red line, creating a new representation, and the opaque red lines stretching from the blue points to the red line indicate the distance between the original datapoint and the new representation. 
HuTao-ghost: Those distances can be thought of as the *information lost* by the projection. The longer those lines, the "farther off" the projection is, and the greater the information loss. 
HuTao-ghost: Move your mouse over the plot to choose a new projection direction. Try to find the direction that incurs minimal information loss. Also pay attention to the variance of the projected datapoints in the 1D line plot below. I'll give you a moment to play around with this interactive visualization. 
HuTao-ghost: 

HuTao-ghost: This is the direction that approximately minimizes the information loss. 
HuTao-ghost: 
HuTao-ghost: Furthermore, the variance of the projections is also maximized. You can go back to a few slides and confirm this. 

HuTao-ghost: ***Maximizing variance is equivalent to minimizing information loss*** This is a very important and interesting conclusion. 

HuTao-ghost: Focusing on the idea of variance, the variance calculation can be represented in a very convenient way. 
HuTao-ghost: If you center the data, the mean will be zero, leading the expression to just be a sum of multiplications between each projected datapoint and itself. 
HuTao-ghost: A sum of multiplications is very similar to a dot product. So similar to the extent that it's computationally identical as the dot product. 
HuTao-ghost: In this case, the Xc u notation is just a matrix multiplication, which calculates the dot product between each centered datapoint and the direction vector, i.e. it represents the projection of each datapoint onto the direction vector.
HuTao-ghost: The transpose notation is computationally equivalent to the dot product. Basically going the other way in the dot product matrix multiplication relationship. 
HuTao-ghost: Finally you can factor in the transpose notation to get this form. 1/n * Xc.T @ Xc is the *covariance matrix* of the data. 
HuTao-ghost: Any matrix multiplied by it's transpose is a symmetric matrix, meaning the covariance matrix, which is a result of such a condition, is symmetric. 
HuTao-ghost: If you recall the spectral theorem from linear algebra, you'll remember that for any symmetric matrix S, the expression u.T @ S @ u, is maximized by the top eigenvector and minimized by the bottom eigenvector.
HuTao-ghost: Going back to the images, we can calculate the covariance matrix, and find the top eigenvectors, which will maximize that expression, which is identical to the variance. 

HuTao-ghost: Out of all the eigenvectors, this one is my favorite. 
Luigi-scared: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA!!!!!!!!!!!!!!!!
HuTao-ghost: Since the eigenvectors correspond roughly to faces, perhaps we can call them... eigenfaces. The term has a cute ring to it. 

Hutao-ghost: Here are the top 30 eigenvectors/eigenfaces. 
Hutao-ghost: 